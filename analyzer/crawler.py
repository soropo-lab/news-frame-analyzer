# -*- coding: utf-8 -*-
"""
ğŸ“° êµ­ë‚´ ì£¼ìš” ì–¸ë¡  ìë™ ë‰´ìŠ¤ ë³¸ë¬¸ ìˆ˜ì§‘ê¸° (v6.1, 2025ë…„ 10ì›” ê¸°ì¤€)
 - ì¡°ì„ ì¼ë³´, JTBC, í•œê²¨ë ˆ, KBS, MBC ìë™ ê°ì§€
 - JSON + HTML êµ¬ì¡° ëŒ€ì‘
 - ê´‘ê³ , ìŠ¤í¬ë¦½íŠ¸, iframe, figure ìë™ ì œê±°
 - Streamlit í™˜ê²½ì—ì„œë„ ë©ˆì¶¤ ì—†ìŒ (Selenium ë¯¸ì‚¬ìš©)
"""

import re
import requests
from bs4 import BeautifulSoup

# -------------------------------------------------
# ğŸ§¼ ê³µí†µ: í…ìŠ¤íŠ¸ ì •ë¦¬ í•¨ìˆ˜
# -------------------------------------------------
def clean_text(text):
    return re.sub(r"\s+", " ", text).strip()


# -------------------------------------------------
# ğŸŒ ê³µí†µ User-Agent
# -------------------------------------------------
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/122.0 Safari/537.36"
}


# -------------------------------------------------
# ğŸ“° ì¡°ì„ ì¼ë³´
# -------------------------------------------------
def get_chosun_text(url):
    """ì¡°ì„ ì¼ë³´ JSON + HTML"""
    # â‘  JSON ì‹œë„
    try:
        m = re.search(r"chosun\.com/(.+?)/(\d{4})/(\d{2})/([A-Z0-9]+)/", url)
        if m:
            section, year, month, article_id = m.groups()
            json_url = f"https://www.chosun.com/__data/fusion/cached/page/article/{section}/{year}/{month}/{article_id}.json"
            res = requests.get(json_url, timeout=10)
            res.raise_for_status()
            data = res.json()
            body_html = (
                data.get("props", {})
                .get("pageProps", {})
                .get("article", {})
                .get("body", "")
            )
            if body_html:
                soup = BeautifulSoup(body_html, "html.parser")
                text = " ".join(p.get_text(strip=True) for p in soup.find_all("p"))
                if len(text) > 200:
                    return clean_text(text)
    except Exception:
        pass

    # â‘¡ HTML ì‹œë„
    try:
        res = requests.get(url, headers=HEADERS, timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        article = soup.select_one(
            "div.article-body, section.article-body, div[data-fusion-container]"
        )
        if article:
            text = " ".join(p.get_text(strip=True) for p in article.find_all("p"))
            return clean_text(text)
    except Exception:
        pass
    return ""


# -------------------------------------------------
# ğŸ“° JTBC (2025ë…„ 10ì›” ìµœì‹  êµ¬ì¡° ëŒ€ì‘)
# -------------------------------------------------
def get_jtbc_text(url):
    """JTBC ë‰´ìŠ¤ ë³¸ë¬¸ ìˆ˜ì§‘"""
    try:
        res = requests.get(url, headers=HEADERS, timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")

        # âœ… ìµœì‹  êµ¬ì¡°: id="ijam_content"
        article = soup.select_one(
            "#ijam_content, #article_content, #article_content_area, .article_content"
        )
        if not article:
            # idì— 'jam_content'ê°€ í¬í•¨ëœ ëª¨ë“  div íƒìƒ‰
            article = soup.find("div", id=lambda x: x and "jam_content" in x)
        if not article:
            return ""

        # ê´‘ê³ /iframe/ìŠ¤í¬ë¦½íŠ¸ ì œê±°
        for tag in article.select(
            "script, iframe, figure, div.ad_area, .set_contents_video_ad, .set_contents_image_ad"
        ):
            tag.decompose()

        # âœ… span, b, p ëª¨ë‘ í¬í•¨í•´ì„œ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
        texts = []
        for tag in article.find_all(["p", "span", "b"]):
            t = tag.get_text(strip=True)
            if t and not t.lower().startswith("advertisement"):
                texts.append(t)

        text = " ".join(texts)
        return clean_text(text)
    except Exception as e:
        return f"__ERROR__: {e}"


# -------------------------------------------------
# ğŸ“° í•œê²¨ë ˆ
# -------------------------------------------------
def get_hani_text(url):
    """í•œê²¨ë ˆ ë³¸ë¬¸"""
    try:
        res = requests.get(url, headers=HEADERS, timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        article = soup.select_one("div.article-text, div.text, #article-text")
        if article:
            text = " ".join(p.get_text(strip=True) for p in article.find_all("p"))
            return clean_text(text)
    except Exception:
        pass
    return ""


# -------------------------------------------------
# ğŸ“° KBS
# -------------------------------------------------
def get_kbs_text(url):
    """KBS ë³¸ë¬¸"""
    try:
        res = requests.get(url, headers=HEADERS, timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        article = soup.select_one("div.detail-body, div.detail_body, .view_cont")
        if article:
            text = " ".join(p.get_text(strip=True) for p in article.find_all("p"))
            return clean_text(text)
    except Exception:
        pass
    return ""


# -------------------------------------------------
# ğŸ“° MBC
# -------------------------------------------------
def get_mbc_text(url):
    """MBC ë³¸ë¬¸"""
    try:
        res = requests.get(url, headers=HEADERS, timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        article = soup.select_one("div.news_cont, div.news_body, div#content")
        if article:
            text = " ".join(p.get_text(strip=True) for p in article.find_all("p"))
            return clean_text(text)
    except Exception:
        pass
    return ""


# -------------------------------------------------
# ğŸ§© ìë™ ê°ì§€ í†µí•© í•¨ìˆ˜
# -------------------------------------------------
def get_article_text(url):
    """URL ê¸°ë°˜ìœ¼ë¡œ ì–¸ë¡ ì‚¬ ê°ì§€ í›„ ë³¸ë¬¸ ìë™ ìˆ˜ì§‘"""
    url = url.strip().lower()

    if "chosun.com" in url:
        text = get_chosun_text(url)
    elif "jtbc.co.kr" in url:
        text = get_jtbc_text(url)
    elif "hani.co.kr" in url:
        text = get_hani_text(url)
    elif "kbs.co.kr" in url:
        text = get_kbs_text(url)
    elif "mbc.co.kr" in url:
        text = get_mbc_text(url)
    else:
        # Fallback: <p> ì „ì²´ ìˆ˜ì§‘
        try:
            res = requests.get(url, headers=HEADERS, timeout=10)
            res.raise_for_status()
            soup = BeautifulSoup(res.text, "html.parser")
            text = " ".join(p.get_text(strip=True) for p in soup.find_all("p"))
            text = clean_text(text)
        except Exception:
            text = ""

    if not text or len(text) < 200:
        return f"__ERROR__: ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨ ({url})"
    return text


# -------------------------------------------------
# ğŸ§ª ë‹¨ë… í…ŒìŠ¤íŠ¸ìš© ì‹¤í–‰
# -------------------------------------------------
if __name__ == "__main__":
    urls = [
        "https://news.jtbc.co.kr/article/NB12265505",
        "https://www.chosun.com/economy/market_trend/2025/09/29/IFEAT6REQBB5NH77T7YYJ2RYX4/",
        "https://www.hani.co.kr/arti/opinion/editorial/1221403.html",
        "https://news.kbs.co.kr/news/pc/view/view.do?ncd=8370213",
        "https://imnews.imbc.com/replay/2025/nwdesk/article/6572301_36192.html",
    ]

    for url in urls:
        print(f"\nâ–¶ {url}")
        text = get_article_text(url)
        if text.startswith("__ERROR__"):
            print(text)
        else:
            print(text[:350] + "...")
